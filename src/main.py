"""Main entry point for the job crawler application."""

import structlog
import sys
from pathlib import Path
from typing import Optional
import getpass

from crawler.main_crawler import JobCrawler
from crawler.core.models import SearchFilters, JobType, ExperienceLevel
from crawler.core.config import get_settings

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="ISO"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


def get_user_credentials(site_name: str) -> tuple[str, str]:
    """Get credentials from user input."""
    print(f"\nğŸ”‘ {site_name.upper()} ê³„ì • ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    username = input(f"{site_name.upper()} ì•„ì´ë””/ì´ë©”ì¼: ").strip()
    password = getpass.getpass(f"{site_name.upper()} ë¹„ë°€ë²ˆí˜¸: ").strip()
    print("âœ… ê³„ì • ì •ë³´ ì…ë ¥ ì™„ë£Œ!")
    return username, password


def get_output_directory() -> Path:
    """Get output directory from user input with cross-platform compatibility."""
    import os
    
    print("\nğŸ“ Excel íŒŒì¼ ì €ì¥ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
    print("ğŸ’¡ íŒ: ê²½ë¡œ ì˜ˆì‹œ:")
    print("   Windows: C:\\Users\\ì‚¬ìš©ìëª…\\Desktop\\ì±„ìš©ê³µê³ ")
    print("   Mac/Linux: /Users/ì‚¬ìš©ìëª…/Desktop/ì±„ìš©ê³µê³  ë˜ëŠ” ~/Desktop/ì±„ìš©ê³µê³ ")
    print("   í˜„ì¬ í´ë”: . (ì  í•˜ë‚˜)")
    
    while True:
        try:
            user_path = input("\nğŸ“‚ ì €ì¥ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„° = í˜„ì¬ í´ë”ì˜ output): ").strip()
            
            # Default to current directory's output folder
            if not user_path:
                output_dir = Path.cwd() / "output"
                print(f"âœ… ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {output_dir.absolute()}")
            else:
                # Expand user path (handles ~ on Mac/Linux)
                expanded_path = os.path.expanduser(user_path)
                output_dir = Path(expanded_path).resolve()
                print(f"âœ… ì„¤ì •ëœ ê²½ë¡œ: {output_dir.absolute()}")
            
            # Create directory if it doesn't exist
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Test write permissions
            test_file = output_dir / "test_write_permission.tmp"
            try:
                test_file.write_text("test")
                test_file.unlink()  # Delete test file
                print(f"âœ… ê²½ë¡œ í™•ì¸ ì™„ë£Œ! íŒŒì¼ ì €ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return output_dir
            except PermissionError:
                print(f"âŒ ê¶Œí•œ ì˜¤ë¥˜: '{output_dir}' ê²½ë¡œì— íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   ë‹¤ë¥¸ ê²½ë¡œë¥¼ ì„ íƒí•˜ê±°ë‚˜ í´ë” ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                print(f"âŒ ê²½ë¡œ ì˜¤ë¥˜: {e}")
                
        except Exception as e:
            print(f"âŒ ì˜ëª»ëœ ê²½ë¡œì…ë‹ˆë‹¤: {e}")
            print("   ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        print("ğŸ”„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”...")


def main():
    """Main entry point for the crawler."""
    try:
        logger.info("Starting job crawler application")
        print("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”~ ì‚°ì ë“œë¦¼~")
        # Get site selection from user
        print("ğŸŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì±„ìš© ì‚¬ì´íŠ¸:")
        print("1. LinkedIn(Coming Soon)")
        print("2. Indeed(Coming Soon)")
        print("3. Work24 (ê³ ìš©24)")
        
        while True:
            choice = input("\nì‚¬ì´íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
            if choice == "1":
                site_name = "linkedin"
                print("âœ… LinkedIn ì„ íƒë¨")
                break
            elif choice == "2":
                site_name = "indeed"
                print("âœ… Indeed ì„ íƒë¨")
                break
            elif choice == "3":
                site_name = "work24"
                print("âœ… Work24 ì„ íƒë¨")
                break
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1, 2, 3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        # Get credentials from user
        username, password = get_user_credentials(site_name)
        
        # Get output directory from user
        output_directory = get_output_directory()
        
        # Get search keywords from user (optional)
        keywords = input("\nğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­, ì—”í„°=í‚¤ì›Œë“œ ì—†ì´ ê²€ìƒ‰): ").strip()
        if keywords:
            print(f"âœ… í‚¤ì›Œë“œ ì„¤ì •: '{keywords}'")
        else:
            print("âœ… í‚¤ì›Œë“œ ì—†ì´ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # Get number of jobs from user
        while True:
            try:
                max_jobs = int(input("\nğŸ“Š ìˆ˜ì§‘í•  ì±„ìš©ê³µê³  ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-1000): ").strip())
                if 1 <= max_jobs <= 1000:
                    print(f"âœ… ìˆ˜ì§‘ ëª©í‘œ: {max_jobs}ê°œ")
                    break
                else:
                    print("âŒ 1ê³¼ 1000 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # Define search criteria
        search_filters = SearchFilters(
            keywords=keywords if keywords else None,
            location="Seoul, Korea" if site_name == "work24" else "New York, NY",
            job_type=JobType.FULL_TIME,
            experience_level=ExperienceLevel.MID_LEVEL,
            date_posted="7d",  # Last 7 days
            remote_only=False,
            max_results=max_jobs
        )
        
        # Initialize crawler with user credentials and output directory
        crawler = JobCrawler(site_name, (username, password), output_directory)
        
        # Start crawling
        excel_file = crawler.crawl_jobs(search_filters)
        
        if excel_file:
            logger.info(f"Crawling completed successfully. Results saved to: {excel_file}")
            
            # Print session statistics
            stats = crawler.get_session_stats()
            print("\n" + "="*50)
            print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
            print("="*50)
            print(f"ğŸ“‹ ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ : {stats['jobs_found']}ê°œ")
            print(f"ğŸ¢ ê³ ìœ  íšŒì‚¬ ìˆ˜: {stats['unique_companies']}ê°œ")
            print(f"ğŸŒ í¬ë¡¤ë§ ì‚¬ì´íŠ¸: {stats['site_crawled']}")
            print(f"ğŸ“ ì£¼ìš” ì§€ì—­: {', '.join(stats['locations'][:5])}")
            print(f"ğŸ“„ Excel íŒŒì¼: {excel_file}")
            print("="*50)
            print("ğŸ‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            logger.error("Crawling failed")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("Crawling interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜: {e}")
        logger.error(f"Application failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()