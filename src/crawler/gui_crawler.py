"""
GUI-compatible wrapper for the main crawler with callback support
"""

from typing import Optional, Callable, List
from pathlib import Path
import structlog

from .main_crawler import JobCrawler
from .core.models import SearchFilters, JobListing
from .core.config import get_settings


class MainCrawler:
    """GUI-compatible crawler wrapper with logging callbacks"""
    
    def __init__(self, site_name: str, username: str, password: str, 
                 filters: SearchFilters, logger_callback: Optional[Callable[[str], None]] = None):
        self.site_name = site_name
        self.username = username  
        self.password = password
        self.filters = filters
        self.logger_callback = logger_callback or (lambda msg: print(msg))
        
        # Initialize the actual crawler
        self.crawler = JobCrawler(
            site_name=site_name,
            user_credentials=(username, password),
            output_directory=get_settings().output_dir
        )
        
        # Set up logging
        self.logger = structlog.get_logger()
        
    def log(self, message: str):
        """Send log message to GUI"""
        self.logger_callback(message)
        self.logger.info(message)
    
    def login(self) -> bool:
        """This method is not needed as JobCrawler handles login internally"""
        self.log(f"ğŸŒ {self.site_name.upper()} ì¤€ë¹„ ì¤‘...")
        return True
    
    def crawl_jobs(self) -> List[JobListing]:
        """Crawl jobs with progress updates"""
        try:
            self.log("ğŸš€ í¬ë¡¤ë§ ì‹œì‘...")
            self.log(f"ğŸ“Š ìµœëŒ€ {self.filters.max_results}ê°œ ì±„ìš©ê³µê³  ìˆ˜ì§‘ ì˜ˆì •")
            
            # The JobCrawler.crawl_jobs method handles everything:
            # - Browser initialization
            # - Login 
            # - Navigation
            # - Search filtering
            # - Job collection
            # - Excel export
            excel_path = self.crawler.crawl_jobs(self.filters)
            
            if excel_path:
                self.log(f"ğŸ‰ í¬ë¡¤ë§ ì„±ê³µ!")
                self.log(f"ğŸ“ Excel íŒŒì¼ ì €ì¥ë¨: {excel_path}")
                
                # Return the collected jobs for display purposes
                return self.crawler.jobs if hasattr(self.crawler, 'jobs') else []
            else:
                self.log("âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
        except Exception as e:
            self.log(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def cleanup(self):
        """Clean up resources"""
        try:
            # JobCrawler handles its own cleanup via the context manager
            self.log("ğŸ§¹ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.log(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")