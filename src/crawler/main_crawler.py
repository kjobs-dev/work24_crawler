"""Main job crawler orchestrator."""

import structlog
from typing import List, Optional, Dict, Any
from pathlib import Path
import random
import time

from .core.base_crawler import StealthCrawler
from .core.auth import AuthenticationManager
from .core.search_engine import SearchEngine
from .core.job_extractor import JobExtractor
from .utils.excel_exporter import ExcelExporter
from .core.models import SearchFilters, JobListing, CrawlerSession
from .core.config import get_settings

logger = structlog.get_logger()


class JobCrawler:
    """Main job crawler that orchestrates the entire crawling process."""
    
    def __init__(self, site_name: str, user_credentials: Optional[tuple[str, str]] = None, output_directory: Optional[Path] = None):
        self.site_name = site_name
        self.settings = get_settings()
        self.jobs: List[JobListing] = []
        self.user_credentials = user_credentials
        
        # Override output directory if provided by user
        if output_directory:
            self.settings.output_dir = output_directory
            logger.info(f"Using custom output directory: {output_directory}")
        
    def crawl_jobs(self, filters: SearchFilters) -> Optional[Path]:
        """
        Main crawling method that coordinates the entire process.
        
        Args:
            filters: Search criteria for job filtering
            
        Returns:
            Path to exported Excel file, or None if crawling failed
        """
        try:
            logger.info(f"Starting job crawl for {self.site_name}")
            print(f"üöÄ {self.site_name.upper()} Ï±ÑÏö©Í≥µÍ≥† ÌÅ¨Î°§ÎßÅÏùÑ ÏãúÏûëÌï©ÎãàÎã§...")
            
            # Initialize Excel file path tracker
            self.excel_file_path = None
            
            with StealthCrawler(self.site_name, filters) as crawler:
                # Initialize components
                print("‚öôÔ∏è ÌÅ¨Î°§Îü¨ Íµ¨ÏÑ± ÏöîÏÜåÎ•º Ï¥àÍ∏∞ÌôîÌïòÎäî Ï§ë...")
                auth_manager = AuthenticationManager(crawler)
                search_engine = SearchEngine(crawler)
                job_extractor = JobExtractor(crawler)
                
                # Step 1: Navigate to site
                print(f"üåê {self.site_name.upper()} ÏÇ¨Ïù¥Ìä∏Î°ú Ïù¥Îèô Ï§ë...")
                crawler.navigate_to_site()
                
                # Step 2: Login if credentials are available
                if self.user_credentials:
                    email, password = self.user_credentials
                    print("üîê ÏÇ¨Ïö©Ïûê Í≥ÑÏ†ïÏúºÎ°ú Î°úÍ∑∏Ïù∏ Ï§ë...")
                    login_success = auth_manager.login(email, password)
                    if not login_success:
                        print("‚ùå Î°úÍ∑∏Ïù∏ Ïã§Ìå®! ÌÅ¨Î°§ÎßÅÏùÑ Ï§ëÎã®Ìï©ÎãàÎã§.")
                        logger.error("Login failed, aborting crawl")
                        return None
                    print("‚úÖ Î°úÍ∑∏Ïù∏ ÏÑ±Í≥µ!")
                else:
                    email, password = auth_manager.get_credentials()
                    if email and password:
                        print("üîê Ï†ÄÏû•Îêú Í≥ÑÏ†ï Ï†ïÎ≥¥Î°ú Î°úÍ∑∏Ïù∏ Ï§ë...")
                        login_success = auth_manager.login(email, password)
                        if not login_success:
                            print("‚ùå Î°úÍ∑∏Ïù∏ Ïã§Ìå®! ÌÅ¨Î°§ÎßÅÏùÑ Ï§ëÎã®Ìï©ÎãàÎã§.")
                            logger.error("Login failed, aborting crawl")
                            return None
                        print("‚úÖ Î°úÍ∑∏Ïù∏ ÏÑ±Í≥µ!")
                    else:
                        print("‚ÑπÔ∏è Í≥ÑÏ†ï Ï†ïÎ≥¥Í∞Ä ÏóÜÏñ¥ Î°úÍ∑∏Ïù∏ ÏóÜÏù¥ ÏßÑÌñâÌï©ÎãàÎã§.")
                        logger.info("No credentials provided, proceeding without login")
                
                # Step 3: Navigate to jobs page and apply filters
                print("üîç Ï±ÑÏö©Í≥µÍ≥† Í≤ÄÏÉâ ÌéòÏù¥ÏßÄÎ°ú Ïù¥Îèô Ï§ë...")
                search_engine.navigate_to_jobs_page()
                print(f"üéØ Í≤ÄÏÉâ ÌïÑÌÑ∞ Ï†ÅÏö© Ï§ë... (ÌÇ§ÏõåÎìú: '{filters.keywords}')")
                search_engine.apply_search_filters(filters)
                
                # Step 4: Crawl job listings with batch saving and duplicate detection
                print(f"üìã Ï±ÑÏö©Í≥µÍ≥† ÏàòÏßë ÏãúÏûë... (ÏµúÎåÄ {filters.max_results}Í∞ú)")
                print(f"üíæ {self.settings.batch_save_size}Í∞úÏî© Î∞∞Ïπò Ï†ÄÏû•ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ Î∞©ÏßÄÌï©ÎãàÎã§.")
                print("üîç Ï§ëÎ≥µ Ï±ÑÏö©Í≥µÍ≥† ÏûêÎèô Ïä§ÌÇµ Í∏∞Îä•Ïù¥ ÌôúÏÑ±ÌôîÎêòÏóàÏäµÎãàÎã§.")
                self._crawl_all_pages(crawler, job_extractor, filters.max_results)
                
                # Step 5: Check results and provide summary
                if self.jobs:
                    print(f"üéâ ÌÅ¨Î°§ÎßÅ ÏÑ±Í≥µ! Ï¥ù {len(self.jobs)}Í∞ú Ï±ÑÏö©Í≥µÍ≥†Î•º ÏàòÏßëÌñàÏäµÎãàÎã§.")
                    print("üìä Î∞∞Ïπò Ï†ÄÏû•ÏúºÎ°ú Î™®Îì† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏïàÏ†ÑÌïòÍ≤å Î≥¥Í¥ÄÎêòÏóàÏäµÎãàÎã§.")
                    
                    # Check if we have the Excel file path from batch saving
                    if hasattr(self, 'excel_file_path') and self.excel_file_path:
                        excel_path = self.excel_file_path
                        
                        # Create summary report (for logging purposes)
                        from crawler.utils.excel_exporter import ExcelExporter
                        exporter = ExcelExporter()
                        summary = exporter.create_summary_report(self.jobs)
                        logger.info(f"Crawl completed: {summary}")
                        print(f"‚úÖ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å! Excel ÌååÏùº: {excel_path.name}")
                        
                        return excel_path
                    else:
                        # Fallback: try to find the most recent Excel file
                        print("üîç Excel ÌååÏùº Í≤ΩÎ°úÎ•º Ï∞æÎäî Ï§ë...")
                        output_dir = self.settings.output_dir
                        excel_files = list(output_dir.glob(f"{self.site_name}_Ï±ÑÏö©Ï†ïÎ≥¥_*.xlsx"))
                        
                        if excel_files:
                            # Get the most recent file
                            excel_path = max(excel_files, key=lambda x: x.stat().st_mtime)
                            print(f"‚úÖ Excel ÌååÏùº Î∞úÍ≤¨: {excel_path.name}")
                            return excel_path
                        else:
                            logger.warning("No Excel files found after crawling")
                            print("‚ö†Ô∏è Excel ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                            return None
                else:
                    print("‚ö†Ô∏è ÏàòÏßëÎêú Ï±ÑÏö©Í≥µÍ≥†Í∞Ä ÏóÜÏäµÎãàÎã§.")
                    logger.warning("No jobs found to export")
                    return None
                    
        except Exception as e:
            print(f"‚ùå ÌÅ¨Î°§ÎßÅ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            logger.error(f"Crawling failed: {e}")
            return None
    
    def _crawl_all_pages(
        self, 
        crawler: StealthCrawler, 
        job_extractor: JobExtractor, 
        max_results: int
    ) -> None:
        """Crawl all pages of search results with configurable batch saving for data safety."""
        page_num = 1
        jobs_processed = 0
        
        # Batch saving configuration
        BATCH_SIZE = self.settings.batch_save_size
        batch_jobs = []
        batch_num = 1
        excel_file_path = None  # Track the Excel file path
        exporter = None  # Initialize when needed
        existing_job_ids = set()  # Track existing job IDs to avoid duplicates
        
        # Generate base filename for this crawl session
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{self.site_name}_Ï±ÑÏö©Ï†ïÎ≥¥_{timestamp}"
        
        # Check if there are existing Excel files to load job IDs from (for duplicate detection across sessions)
        try:
            output_dir = self.settings.output_dir
            existing_excel_files = list(output_dir.glob(f"{self.site_name}_Ï±ÑÏö©Ï†ïÎ≥¥_*.xlsx"))
            if existing_excel_files:
                # Get the most recent file to load existing job IDs
                latest_file = max(existing_excel_files, key=lambda x: x.stat().st_mtime)
                try:
                    import pandas as pd
                    existing_df = pd.read_excel(latest_file, sheet_name="Ï±ÑÏö©Í≥µÍ≥†")
                    if "Ï±ÑÏö©ID" in existing_df.columns:
                        existing_job_ids = set(existing_df["Ï±ÑÏö©ID"].dropna().astype(str))
                        print(f"üîç Í∏∞Ï°¥ Excel ÌååÏùºÏóêÏÑú {len(existing_job_ids)}Í∞ú Ï±ÑÏö©Í≥µÍ≥† IDÎ•º Î°úÎìúÌñàÏäµÎãàÎã§.")
                        logger.info(f"Loaded {len(existing_job_ids)} existing job IDs for duplicate detection")
                except Exception as load_error:
                    logger.warning(f"Could not load existing job IDs: {load_error}")
                    existing_job_ids = set()
        except Exception:
            existing_job_ids = set()
        
        while jobs_processed < max_results:
            try:
                print(f"üìÑ {page_num}ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨ Ï§ë...")
                logger.info(f"Processing page {page_num}")
                
                # Save current page URL for navigation back
                current_search_url = crawler.driver.current_url
                print(f"üîó ÌòÑÏû¨ Í≤ÄÏÉâ Í≤∞Í≥º ÌéòÏù¥ÏßÄ URL Ï†ÄÏû•: {current_search_url}")
                
                # Extract job listings from current page
                print("üîç ÌòÑÏû¨ ÌéòÏù¥ÏßÄÏóêÏÑú Ï±ÑÏö©Í≥µÍ≥† Î™©Î°ù Ï∂îÏ∂ú Ï§ë...")
                job_listings = job_extractor.extract_job_listings_from_search()
                
                if not job_listings:
                    print(f"‚ö†Ô∏è {page_num}ÌéòÏù¥ÏßÄÏóêÏÑú Ï±ÑÏö©Í≥µÍ≥†Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                    logger.warning(f"No jobs found on page {page_num}")
                    break
                
                print(f"üìã {len(job_listings)}Í∞úÏùò Ï±ÑÏö©Í≥µÍ≥†Î•º Î∞úÍ≤¨ÌñàÏäµÎãàÎã§.")
                
                # Process each job listing
                for i, job_data in enumerate(job_listings, 1):
                    if jobs_processed >= max_results:
                        break
                    
                    try:
                        print(f"üìù Ï±ÑÏö©Í≥µÍ≥† {jobs_processed + 1}/{max_results} Ï≤òÎ¶¨ Ï§ë... ({i}/{len(job_listings)})")
                        
                        # Extract detailed job information
                        print("üîç ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
                        detailed_job = job_extractor.extract_detailed_job_info(
                            job_data["job_url"], job_data
                        )
                        
                        if detailed_job:
                            # Check for duplicate job ID
                            if detailed_job.job_id and detailed_job.job_id in existing_job_ids:
                                print(f"‚ö†Ô∏è Ï§ëÎ≥µ Ï±ÑÏö©Í≥µÍ≥† Ïä§ÌÇµ: {detailed_job.job_id} - {detailed_job.title}")
                                logger.info(f"Skipping duplicate job ID: {detailed_job.job_id}")
                                continue
                            
                            # Add to current batch
                            batch_jobs.append(detailed_job)
                            # Also keep in main list for backward compatibility
                            self.jobs.append(detailed_job)
                            # Track this job ID
                            if detailed_job.job_id:
                                existing_job_ids.add(detailed_job.job_id)
                            
                            jobs_processed += 1
                            print(f"‚úÖ ÏôÑÎ£å: {detailed_job.title} - {detailed_job.company}")
                            
                            logger.debug(
                                f"Processed job {jobs_processed}/{max_results}: "
                                f"{detailed_job.title} at {detailed_job.company}"
                            )
                            
                            # Check if we need to save a batch
                            if len(batch_jobs) >= BATCH_SIZE:
                                # Initialize exporter if needed
                                if exporter is None:
                                    from crawler.utils.excel_exporter import ExcelExporter
                                    exporter = ExcelExporter()
                                
                                try:
                                    if batch_num == 1:
                                        # First batch - create new file
                                        print(f"üíæ Ï≤´ Î≤àÏß∏ Î∞∞Ïπò Ï†ÄÏû• Ï§ë... ({BATCH_SIZE}Í∞ú Ï±ÑÏö©Í≥µÍ≥†)")
                                        excel_file_path = exporter.save_incremental_batch(
                                            batch_jobs, batch_num, base_filename, is_first_batch=True
                                        )
                                        print(f"‚úÖ Î∞∞Ïπò {batch_num} Ï†ÄÏû• ÏôÑÎ£å: {excel_file_path.name}")
                                    else:
                                        # Subsequent batches - append
                                        print(f"üíæ Î∞∞Ïπò {batch_num} Ï∂îÍ∞Ä Ï†ÄÏû• Ï§ë... ({len(batch_jobs)}Í∞ú Ï±ÑÏö©Í≥µÍ≥†)")
                                        exporter.save_incremental_batch(
                                            batch_jobs, batch_num, base_filename, is_first_batch=False
                                        )
                                        print(f"‚úÖ Î∞∞Ïπò {batch_num} Ï†ÄÏû• ÏôÑÎ£å")
                                    
                                    # Clear batch and increment counter
                                    batch_jobs = []
                                    batch_num += 1
                                    
                                    logger.info(f"Successfully saved batch {batch_num - 1} with {BATCH_SIZE} jobs")
                                    
                                except Exception as batch_save_error:
                                    print(f"‚ùå Î∞∞Ïπò Ï†ÄÏû• Ïã§Ìå®: {batch_save_error}")
                                    logger.error(f"Failed to save batch {batch_num}: {batch_save_error}")
                                    # Continue crawling even if batch save fails
                            
                        else:
                            print("‚ö†Ô∏è ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå®")
                        
                        # Navigate back to search results page after each job
                        print("üîô Í≤ÄÏÉâ Í≤∞Í≥º ÌéòÏù¥ÏßÄÎ°ú ÎèåÏïÑÍ∞ÄÎäî Ï§ë...")
                        crawler.driver.get(current_search_url)
                        crawler.human_behavior.random_delay(1.5, 2.5)  # Wait for page load
                        
                        # Human-like delay between job extractions (reduced for speed)
                        print("‚è≥ Ïû†Ïãú ÎåÄÍ∏∞ Ï§ë...")
                        crawler.human_behavior.random_delay(
                            max(1.0, self.settings.crawl_delay_min - 1.0),  # Reduced delay
                            max(2.0, self.settings.crawl_delay_max - 2.0)   # Reduced delay
                        )
                        
                    except Exception as e:
                        print(f"‚ùå Ï±ÑÏö©Í≥µÍ≥† Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
                        logger.warning(f"Failed to process job listing: {e}")
                        crawler.session.add_error()
                        # Navigate back to search results in case of error
                        try:
                            crawler.driver.get(current_search_url)
                            crawler.human_behavior.random_delay(1.0, 2.0)
                        except:
                            pass
                        continue
                
                # Check if we should continue to next page
                if jobs_processed < max_results:
                    print(f"üìä ÏßÑÌñâ ÏÉÅÌô©: {jobs_processed}/{max_results} ÏôÑÎ£å, Îã§Ïùå ÌéòÏù¥ÏßÄ ÌôïÏù∏ Ï§ë...")
                    
                    from crawler.core.search_engine import SearchEngine
                    search_engine = SearchEngine(crawler)
                    
                    if search_engine.has_next_page():
                        print("‚û°Ô∏è Îã§Ïùå ÌéòÏù¥ÏßÄÎ°ú Ïù¥Îèô Ï§ë...")
                        
                        if search_engine.go_to_next_page():
                            page_num += 1
                            print(f"‚úÖ {page_num}ÌéòÏù¥ÏßÄÎ°ú Ïù¥Îèô ÏôÑÎ£å")
                            # Shorter pause between pages for speed
                            crawler.human_behavior.random_delay(2.0, 4.0)  # Reduced from 3.0-7.0
                        else:
                            print("‚ùå Îã§Ïùå ÌéòÏù¥ÏßÄÎ°ú Ïù¥ÎèôÌï† Ïàò ÏóÜÏäµÎãàÎã§. ÌÅ¨Î°§ÎßÅÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§.")
                            logger.info("Could not navigate to next page, ending crawl")
                            break
                    else:
                        print("üìÑ Îçî Ïù¥ÏÉÅ ÌéòÏù¥ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
                        logger.info("No more pages available")
                        break
                else:
                    print(f"üéØ Î™©Ìëú ÏàòÏßëÎüâ Îã¨ÏÑ±! ({max_results}Í∞ú)")
                    logger.info(f"Reached maximum results limit: {max_results}")
                    break
                    
            except Exception as e:
                print(f"‚ùå {page_num}ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
                logger.error(f"Error processing page {page_num}: {e}")
                break
        
        # Save any remaining jobs in the final batch
        if batch_jobs:
            # Initialize exporter if needed
            if exporter is None:
                from crawler.utils.excel_exporter import ExcelExporter
                exporter = ExcelExporter()
            
            try:
                print(f"üíæ ÎßàÏßÄÎßâ Î∞∞Ïπò Ï†ÄÏû• Ï§ë... ({len(batch_jobs)}Í∞ú Ï±ÑÏö©Í≥µÍ≥†)")
                if batch_num == 1:
                    # Only one partial batch - create new file
                    excel_file_path = exporter.save_incremental_batch(
                        batch_jobs, batch_num, base_filename, is_first_batch=True
                    )
                    print(f"‚úÖ ÏµúÏ¢Ö Î∞∞Ïπò Ï†ÄÏû• ÏôÑÎ£å: {excel_file_path.name}")
                else:
                    # Append remaining jobs
                    exporter.save_incremental_batch(
                        batch_jobs, batch_num, base_filename, is_first_batch=False
                    )
                    print(f"‚úÖ ÏµúÏ¢Ö Î∞∞Ïπò {batch_num} Ï†ÄÏû• ÏôÑÎ£å")
                
                logger.info(f"Successfully saved final batch with {len(batch_jobs)} jobs")
                
            except Exception as final_batch_error:
                print(f"‚ùå ÏµúÏ¢Ö Î∞∞Ïπò Ï†ÄÏû• Ïã§Ìå®: {final_batch_error}")
                logger.error(f"Failed to save final batch: {final_batch_error}")
        
        # Store the Excel file path for main crawler to find
        if excel_file_path:
            self.excel_file_path = excel_file_path
        
        print(f"üèÅ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å! Ï¥ù {jobs_processed}Í∞ú Ï±ÑÏö©Í≥µÍ≥†Î•º {page_num}ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÏßëÌñàÏäµÎãàÎã§.")
        if batch_num > 1 or batch_jobs:
            total_batches = batch_num - 1 + (1 if batch_jobs else 0)
            print(f"üíæ Ï¥ù {total_batches}Í∞ú Î∞∞ÏπòÎ°ú ÏïàÏ†ÑÌïòÍ≤å Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
            if excel_file_path:
                print(f"üìÑ Excel ÌååÏùº: {excel_file_path.name}")
        logger.info(f"Crawling completed. Processed {jobs_processed} jobs across {page_num} pages")
    
    def get_session_stats(self) -> Dict[str, Any]:
        """Get statistics about the current crawling session."""
        return {
            "jobs_found": len(self.jobs),
            "site_crawled": self.site_name,
            "unique_companies": len(set(job.company for job in self.jobs)),
            "locations": list(set(job.location for job in self.jobs))[:10],  # Top 10
            "avg_jobs_per_company": len(self.jobs) / len(set(job.company for job in self.jobs)) if self.jobs else 0
        }