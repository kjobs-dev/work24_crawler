"""Main job crawler orchestrator."""

import structlog
from typing import List, Optional, Dict, Any
from pathlib import Path
import random
import time

from .core.base_crawler import StealthCrawler
from .core.auth import AuthenticationManager
from .core.search_engine import SearchEngine
from .core.job_extractor import JobExtractor
from .utils.excel_exporter import ExcelExporter
from .core.models import SearchFilters, JobListing, CrawlerSession
from .core.config import get_settings

logger = structlog.get_logger()


class JobCrawler:
    """Main job crawler that orchestrates the entire crawling process."""
    
    def __init__(self, site_name: str, user_credentials: Optional[tuple[str, str]] = None, output_directory: Optional[Path] = None):
        self.site_name = site_name
        self.settings = get_settings()
        self.jobs: List[JobListing] = []
        self.user_credentials = user_credentials
        
        # Override output directory if provided by user
        if output_directory:
            self.settings.output_dir = output_directory
            logger.info(f"Using custom output directory: {output_directory}")
        
    def crawl_jobs(self, filters: SearchFilters) -> Optional[Path]:
        """
        Main crawling method that coordinates the entire process.
        
        Args:
            filters: Search criteria for job filtering
            
        Returns:
            Path to exported Excel file, or None if crawling failed
        """
        try:
            logger.info(f"Starting job crawl for {self.site_name}")
            print(f"ğŸš€ {self.site_name.upper()} ì±„ìš©ê³µê³  í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # Initialize Excel file path tracker
            self.excel_file_path = None
            
            with StealthCrawler(self.site_name, filters) as crawler:
                # Initialize components
                print("âš™ï¸ í¬ë¡¤ëŸ¬ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
                auth_manager = AuthenticationManager(crawler)
                search_engine = SearchEngine(crawler)
                job_extractor = JobExtractor(crawler)
                
                # Step 1: Navigate to site
                print(f"ğŸŒ {self.site_name.upper()} ì‚¬ì´íŠ¸ë¡œ ì´ë™ ì¤‘...")
                crawler.navigate_to_site()
                
                # Step 2: Login if credentials are available
                if self.user_credentials:
                    email, password = self.user_credentials
                    print("ğŸ” ì‚¬ìš©ì ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ ì¤‘...")
                    login_success = auth_manager.login(email, password)
                    if not login_success:
                        print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨! í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        logger.error("Login failed, aborting crawl")
                        return None
                    print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                else:
                    email, password = auth_manager.get_credentials()
                    if email and password:
                        print("ğŸ” ì €ì¥ëœ ê³„ì • ì •ë³´ë¡œ ë¡œê·¸ì¸ ì¤‘...")
                        login_success = auth_manager.login(email, password)
                        if not login_success:
                            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨! í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            logger.error("Login failed, aborting crawl")
                            return None
                        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                    else:
                        print("â„¹ï¸ ê³„ì • ì •ë³´ê°€ ì—†ì–´ ë¡œê·¸ì¸ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
                        logger.info("No credentials provided, proceeding without login")
                
                # Step 3: Navigate to jobs page and apply filters
                print("ğŸ” ì±„ìš©ê³µê³  ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                search_engine.navigate_to_jobs_page()
                print(f"ğŸ¯ ê²€ìƒ‰ í•„í„° ì ìš© ì¤‘... (í‚¤ì›Œë“œ: '{filters.keywords}')")
                search_engine.apply_search_filters(filters)
                
                # Step 4: Crawl job listings with batch saving and duplicate detection
                print(f"ğŸ“‹ ì±„ìš©ê³µê³  ìˆ˜ì§‘ ì‹œì‘... (ìµœëŒ€ {filters.max_results}ê°œ)")
                print(f"ğŸ’¾ {self.settings.batch_save_size}ê°œì”© ë°°ì¹˜ ì €ì¥ìœ¼ë¡œ ë°ì´í„° ì†ì‹¤ì„ ë°©ì§€í•©ë‹ˆë‹¤.")
                print("ğŸ” ì¤‘ë³µ ì±„ìš©ê³µê³  ìë™ ìŠ¤í‚µ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                self._crawl_all_pages(crawler, job_extractor, filters.max_results)
                
                # Step 5: Check results and provide summary
                if self.jobs:
                    print(f"ğŸ‰ í¬ë¡¤ë§ ì„±ê³µ! ì´ {len(self.jobs)}ê°œ ì±„ìš©ê³µê³ ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    print("ğŸ“Š ë°°ì¹˜ ì €ì¥ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ê°€ ì•ˆì „í•˜ê²Œ ë³´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    # Check if we have the Excel file path from batch saving
                    if hasattr(self, 'excel_file_path') and self.excel_file_path:
                        excel_path = self.excel_file_path
                        
                        # Create summary report (for logging purposes)
                        from crawler.utils.excel_exporter import ExcelExporter
                        exporter = ExcelExporter()
                        summary = exporter.create_summary_report(self.jobs)
                        logger.info(f"Crawl completed: {summary}")
                        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! Excel íŒŒì¼: {excel_path.name}")
                        
                        return excel_path
                    else:
                        # Fallback: try to find the most recent Excel file
                        print("ğŸ” Excel íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ëŠ” ì¤‘...")
                        output_dir = self.settings.output_dir
                        excel_files = list(output_dir.glob(f"{self.site_name}_ì±„ìš©ì •ë³´_*.xlsx"))
                        
                        if excel_files:
                            # Get the most recent file
                            excel_path = max(excel_files, key=lambda x: x.stat().st_mtime)
                            print(f"âœ… Excel íŒŒì¼ ë°œê²¬: {excel_path.name}")
                            return excel_path
                        else:
                            logger.warning("No Excel files found after crawling")
                            print("âš ï¸ Excel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return None
                else:
                    print("âš ï¸ ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    logger.warning("No jobs found to export")
                    return None
                    
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"Crawling failed: {e}")
            return None
    
    def _crawl_all_pages(
        self, 
        crawler: StealthCrawler, 
        job_extractor: JobExtractor, 
        max_results: int
    ) -> None:
        """Crawl all pages of search results with configurable batch saving for data safety."""
        page_num = 1
        jobs_processed = 0
        
        # Batch saving configuration
        BATCH_SIZE = self.settings.batch_save_size
        batch_jobs = []
        batch_num = 1
        excel_file_path = None  # Track the Excel file path
        exporter = None  # Initialize when needed
        existing_job_ids = set()  # Track existing job IDs to avoid duplicates
        
        # Generate base filename for this crawl session
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{self.site_name}_ì±„ìš©ì •ë³´_{timestamp}"
        
        # Check if there are existing Excel files to load job IDs from (for duplicate detection across sessions)
        try:
            output_dir = self.settings.output_dir
            existing_excel_files = list(output_dir.glob(f"{self.site_name}_ì±„ìš©ì •ë³´_*.xlsx"))
            if existing_excel_files:
                # Get the most recent file to load existing job IDs
                latest_file = max(existing_excel_files, key=lambda x: x.stat().st_mtime)
                try:
                    import pandas as pd
                    existing_df = pd.read_excel(latest_file, sheet_name="ì±„ìš©ê³µê³ ")
                    if "ì±„ìš©ID" in existing_df.columns:
                        existing_job_ids = set(existing_df["ì±„ìš©ID"].dropna().astype(str))
                        print(f"ğŸ” ê¸°ì¡´ Excel íŒŒì¼ì—ì„œ {len(existing_job_ids)}ê°œ ì±„ìš©ê³µê³  IDë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                        logger.info(f"Loaded {len(existing_job_ids)} existing job IDs for duplicate detection")
                except Exception as load_error:
                    logger.warning(f"Could not load existing job IDs: {load_error}")
                    existing_job_ids = set()
        except Exception:
            existing_job_ids = set()
        
        while jobs_processed < max_results:
            try:
                print(f"ğŸ“„ {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
                logger.info(f"Processing page {page_num}")
                
                # Save current page URL for navigation back
                current_search_url = crawler.driver.current_url
                print(f"ğŸ”— í˜„ì¬ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ URL ì €ì¥: {current_search_url}")
                
                # Extract job listings from current page
                print("ğŸ” í˜„ì¬ í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³  ëª©ë¡ ì¶”ì¶œ ì¤‘...")
                job_listings = job_extractor.extract_job_listings_from_search()
                
                if not job_listings:
                    print(f"âš ï¸ {page_num}í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    logger.warning(f"No jobs found on page {page_num}")
                    break
                
                print(f"ğŸ“‹ {len(job_listings)}ê°œì˜ ì±„ìš©ê³µê³ ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                
                # Process each job listing
                for i, job_data in enumerate(job_listings, 1):
                    if jobs_processed >= max_results:
                        break
                    
                    try:
                        print(f"ğŸ“ ì±„ìš©ê³µê³  {jobs_processed + 1}/{max_results} ì²˜ë¦¬ ì¤‘... ({i}/{len(job_listings)})")
                        
                        # Extract detailed job information
                        print("ğŸ” ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                        detailed_job = job_extractor.extract_detailed_job_info(
                            job_data["job_url"], job_data
                        )
                        
                        if detailed_job:
                            # Check for duplicate job ID
                            if detailed_job.job_id and detailed_job.job_id in existing_job_ids:
                                print(f"âš ï¸ ì¤‘ë³µ ì±„ìš©ê³µê³  ìŠ¤í‚µ: {detailed_job.job_id} - {detailed_job.title}")
                                logger.info(f"Skipping duplicate job ID: {detailed_job.job_id}")
                                continue
                            
                            # Add to current batch
                            batch_jobs.append(detailed_job)
                            # Also keep in main list for backward compatibility
                            self.jobs.append(detailed_job)
                            # Track this job ID
                            if detailed_job.job_id:
                                existing_job_ids.add(detailed_job.job_id)
                            
                            jobs_processed += 1
                            print(f"âœ… ì™„ë£Œ: {detailed_job.title} - {detailed_job.company}")
                            
                            logger.debug(
                                f"Processed job {jobs_processed}/{max_results}: "
                                f"{detailed_job.title} at {detailed_job.company}"
                            )
                            
                            # Check if we need to save a batch
                            if len(batch_jobs) >= BATCH_SIZE:
                                # Initialize exporter if needed
                                if exporter is None:
                                    from crawler.utils.excel_exporter import ExcelExporter
                                    exporter = ExcelExporter()
                                
                                try:
                                    if batch_num == 1:
                                        # First batch - create new file
                                        print(f"ğŸ’¾ ì²« ë²ˆì§¸ ë°°ì¹˜ ì €ì¥ ì¤‘... ({BATCH_SIZE}ê°œ ì±„ìš©ê³µê³ )")
                                        excel_file_path = exporter.save_incremental_batch(
                                            batch_jobs, batch_num, base_filename, is_first_batch=True
                                        )
                                        print(f"âœ… ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ: {excel_file_path.name}")
                                    else:
                                        # Subsequent batches - append
                                        print(f"ğŸ’¾ ë°°ì¹˜ {batch_num} ì¶”ê°€ ì €ì¥ ì¤‘... ({len(batch_jobs)}ê°œ ì±„ìš©ê³µê³ )")
                                        exporter.save_incremental_batch(
                                            batch_jobs, batch_num, base_filename, is_first_batch=False
                                        )
                                        print(f"âœ… ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ")
                                    
                                    # Clear batch and increment counter
                                    batch_jobs = []
                                    batch_num += 1
                                    
                                    logger.info(f"Successfully saved batch {batch_num - 1} with {BATCH_SIZE} jobs")
                                    
                                except Exception as batch_save_error:
                                    print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {batch_save_error}")
                                    logger.error(f"Failed to save batch {batch_num}: {batch_save_error}")
                                    # Continue crawling even if batch save fails
                            
                        else:
                            print("âš ï¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                        # Navigate back to search results page after each job
                        print("ğŸ”™ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë¡œ ëŒì•„ê°€ëŠ” ì¤‘...")
                        crawler.driver.get(current_search_url)
                        crawler.human_behavior.random_delay(1.5, 2.5)  # Wait for page load
                        
                        # Human-like delay between job extractions (reduced for speed)
                        print("â³ ì ì‹œ ëŒ€ê¸° ì¤‘...")
                        crawler.human_behavior.random_delay(
                            max(1.0, self.settings.crawl_delay_min - 1.0),  # Reduced delay
                            max(2.0, self.settings.crawl_delay_max - 2.0)   # Reduced delay
                        )
                        
                    except Exception as e:
                        print(f"âŒ ì±„ìš©ê³µê³  ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        logger.warning(f"Failed to process job listing: {e}")
                        crawler.session.add_error()
                        # Navigate back to search results in case of error
                        try:
                            crawler.driver.get(current_search_url)
                            crawler.human_behavior.random_delay(1.0, 2.0)
                        except:
                            pass
                        continue
                
                # Check if we should continue to next page
                if jobs_processed < max_results:
                    print(f"ğŸ“Š ì§„í–‰ ìƒí™©: {jobs_processed}/{max_results} ì™„ë£Œ, ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì¤‘...")
                    
                    from crawler.core.search_engine import SearchEngine
                    search_engine = SearchEngine(crawler)
                    
                    if search_engine.has_next_page():
                        print("â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                        
                        if search_engine.go_to_next_page():
                            page_num += 1
                            print(f"âœ… {page_num}í˜ì´ì§€ë¡œ ì´ë™ ì™„ë£Œ")
                            # Shorter pause between pages for speed
                            crawler.human_behavior.random_delay(2.0, 4.0)  # Reduced from 3.0-7.0
                        else:
                            print("âŒ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                            logger.info("Could not navigate to next page, ending crawl")
                            break
                    else:
                        print("ğŸ“„ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        logger.info("No more pages available")
                        break
                else:
                    print(f"ğŸ¯ ëª©í‘œ ìˆ˜ì§‘ëŸ‰ ë‹¬ì„±! ({max_results}ê°œ)")
                    logger.info(f"Reached maximum results limit: {max_results}")
                    break
                    
            except Exception as e:
                print(f"âŒ {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                logger.error(f"Error processing page {page_num}: {e}")
                break
        
        # Save any remaining jobs in the final batch
        if batch_jobs:
            # Initialize exporter if needed
            if exporter is None:
                from crawler.utils.excel_exporter import ExcelExporter
                exporter = ExcelExporter()
            
            try:
                print(f"ğŸ’¾ ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì¤‘... ({len(batch_jobs)}ê°œ ì±„ìš©ê³µê³ )")
                if batch_num == 1:
                    # Only one partial batch - create new file
                    excel_file_path = exporter.save_incremental_batch(
                        batch_jobs, batch_num, base_filename, is_first_batch=True
                    )
                    print(f"âœ… ìµœì¢… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {excel_file_path.name}")
                else:
                    # Append remaining jobs
                    exporter.save_incremental_batch(
                        batch_jobs, batch_num, base_filename, is_first_batch=False
                    )
                    print(f"âœ… ìµœì¢… ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ")
                
                logger.info(f"Successfully saved final batch with {len(batch_jobs)} jobs")
                
            except Exception as final_batch_error:
                print(f"âŒ ìµœì¢… ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {final_batch_error}")
                logger.error(f"Failed to save final batch: {final_batch_error}")
        
        # Store the Excel file path for main crawler to find
        if excel_file_path:
            self.excel_file_path = excel_file_path
        
        print(f"ğŸ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {jobs_processed}ê°œ ì±„ìš©ê³µê³ ë¥¼ {page_num}í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        if batch_num > 1 or batch_jobs:
            total_batches = batch_num - 1 + (1 if batch_jobs else 0)
            print(f"ğŸ’¾ ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if excel_file_path:
                print(f"ğŸ“„ Excel íŒŒì¼: {excel_file_path.name}")
        logger.info(f"Crawling completed. Processed {jobs_processed} jobs across {page_num} pages")
    
    def get_session_stats(self) -> Dict[str, Any]:
        """Get statistics about the current crawling session."""
        return {
            "jobs_found": len(self.jobs),
            "site_crawled": self.site_name,
            "unique_companies": len(set(job.company for job in self.jobs)),
            "locations": list(set(job.location for job in self.jobs))[:10],  # Top 10
            "avg_jobs_per_company": len(self.jobs) / len(set(job.company for job in self.jobs)) if self.jobs else 0
        }